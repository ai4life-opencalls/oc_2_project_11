{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f363dc7e",
   "metadata": {},
   "source": [
    "# Automatica Mask Generator\n",
    "\n",
    "The main goal of this notebook will be to assess the value of using automatic mask generation to find objects or descriptors of the images, such as the complexity of the seabed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560725a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "from utils import (\n",
    "    download_model,\n",
    "    get_point_coord,\n",
    "    get_polygon,\n",
    "    show_points,\n",
    "    show_mask,\n",
    "    show_res_multi,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(annotation_name, image_dir):\n",
    "    image_name = \"_\".join(annotation_name.split(\"_\")[:-1])\n",
    "    image_file = image_dir.joinpath(image_name + \".jpg\")\n",
    "    if image_file.exists():\n",
    "        image = Image.open(image_file)\n",
    "        return np.array(image), image_file\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def save_image_masks(masks, image_name, results_dir):\n",
    "    save_dir = results_dir.joinpath(image_name)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, mask in enumerate(masks):\n",
    "        # mask is 3D: 1, y, x\n",
    "        mask_img = mask[0].astype(np.uint8) * 255\n",
    "        mask_img = Image.fromarray(mask_img)\n",
    "        mask_img.save(save_dir.joinpath(f\"{i:03d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    img_area = img.shape[0] * img.shape[1]\n",
    "    for ann in sorted_anns:\n",
    "        if ann[\"area\"] > img_area / 2:\n",
    "            continue\n",
    "\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a2f1b",
   "metadata": {},
   "source": [
    "The following function will download the large SAM2 model's weights from here only if the folder has no model downloaded:\n",
    "\n",
    "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "\n",
    "For all available models see here: https://github.com/facebookresearch/sam2?tab=readme-ov-file#download-checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdebb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, core\n",
    "\n",
    "core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# large sam2: works on gpu > 8g\n",
    "sam2_checkpoint = \"../models/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "config_dir = \"../models/\"\n",
    "\n",
    "# base sam2: smaller version\n",
    "#sam2_checkpoint = \"../../SAM2_models/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "#model_cfg = \"sam2_hiera_b+.yaml\"\n",
    "\n",
    "with initialize(version_base=None, config_path=config_dir):\n",
    "    sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "\n",
    "#predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60704b",
   "metadata": {},
   "source": [
    "You can choose the folder from where the images to be analyzed are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db09425",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path(\"../results/paparazzi_results\")\n",
    "print(image_dir.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6688ab",
   "metadata": {},
   "source": [
    "And here you can choose the file to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6da958",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"../results/paparazzi_results/VID_01_2023_GP__0.14.45.00.jpg\"\n",
    "\n",
    "image = np.array(\n",
    "    Image.open(image_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_per_side: Optional[int] = 32,\n",
    "# points_per_batch: int = 64,\n",
    "# pred_iou_thresh: float = 0.88,\n",
    "# stability_score_thresh: float = 0.95,\n",
    "# stability_score_offset: float = 1.0,\n",
    "# box_nms_thresh: float = 0.7,\n",
    "# crop_n_layers: int = 0,\n",
    "# crop_nms_thresh: float = 0.7,\n",
    "# crop_overlap_ratio: float = 512 / 1500,\n",
    "# crop_n_points_downscale_factor: int = 1,\n",
    "# point_grids: Optional[List[np.ndarray]] = None,\n",
    "# min_mask_region_area: int = 0,\n",
    "# output_mode: str = \"binary_mask\",\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2_model,\n",
    "    points_per_side=64,\n",
    "    pred_iou_thresh=0.87,\n",
    "    stability_score_thresh=0.80,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=3,\n",
    "    crop_nms_thresh=0.7,\n",
    "    min_mask_region_area=500,  # Requires open-cv to run post-processing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d647e7",
   "metadata": {},
   "source": [
    "The following cell will run the model on the image and might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09637b2",
   "metadata": {},
   "source": [
    "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
    "* `segmentation` : the mask\n",
    "* `area` : the area of the mask in pixels\n",
    "* `bbox` : the boundary box of the mask in XYWH format\n",
    "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
    "* `point_coords` : the sampled input point that generated this mask\n",
    "* `stability_score` : an additional measure of mask quality\n",
    "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(masks[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979acff",
   "metadata": {},
   "source": [
    "We can import the annotations file to compare the masks with what was tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = image_file[:-4] + \"_2704x1520.txt\"\n",
    "\n",
    "df_annotations = pd.read_csv(\n",
    "        file, delimiter=\"\\t\", header=None, names=[\"x\", \"y\", \"label\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_from_point(x, y):\n",
    "    labels = []\n",
    "    for label, mask in enumerate(masks):\n",
    "        if mask[\"segmentation\"][int(y), int(x)]:\n",
    "            labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations[\"masks\"] = df_annotations.apply(lambda x: get_masks_from_point(x.x, x.y), axis=1)\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927f699",
   "metadata": {},
   "source": [
    "You can use the following interactive cells to scroll through the points and see what has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from skimage.morphology import label, remove_small_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2321b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(row=np.arange(0, len(df_annotations)+1, 1))\n",
    "def plot(row):\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(df_annotations.loc[row, \"x\"], df_annotations.loc[row, \"y\"])\n",
    "    for label in df_annotations.loc[row, \"masks\"]:\n",
    "        plt.contour(masks[label][\"segmentation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f74c",
   "metadata": {},
   "source": [
    "## Process masks\n",
    "\n",
    "Several masks have been automatically segmented.\n",
    "Some of these masks correspond to the seabed and some are very noisy as they correspond to out-of-focus regions of the image.\n",
    "We should have a way to determine which masks correspond to each of these so we can filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad63986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_coverage(mask):\n",
    "    return np.sum(mask) / (mask.shape[0] * mask.shape[1])\n",
    "\n",
    "\n",
    "def touches_border(mask):\n",
    "    border_percentage = {\n",
    "        \"left\": np.sum(mask[:, 0]) / mask.shape[0],\n",
    "        \"right\": np.sum(mask[:, -1]) / mask.shape[0],\n",
    "        \"top\": np.sum(mask[0]) / mask.shape[1],\n",
    "        \"bottom\": np.sum(mask[-1]) / mask.shape[1],\n",
    "    }\n",
    "    return border_percentage\n",
    "\n",
    "\n",
    "def get_number_of_objects(mask, size_limit=3000):\n",
    "    labeled = label(mask)\n",
    "    number_of_objects = {\"all\": len(np.unique(labeled)) - 1}\n",
    "    labeled = remove_small_objects(labeled, size_limit)\n",
    "    number_of_objects[\"big\"] = len(np.unique(labeled)) - 1\n",
    "    return number_of_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in masks:\n",
    "    mask[\"area_coverage\"] = mask[\"area\"] / (mask[\"segmentation\"].shape[0] * mask[\"segmentation\"].shape[1])\n",
    "    mask[\"touches_border\"] = touches_border(mask[\"segmentation\"])\n",
    "    mask[\"number_of_objects\"] = get_number_of_objects(mask[\"segmentation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, mask in enumerate(masks):\n",
    "    if mask[\"number_of_objects\"][\"big\"] > 1:\n",
    "        print(n, mask[\"number_of_objects\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898eda1",
   "metadata": {},
   "source": [
    "You can interactively go through the masks with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9336f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(row=np.arange(0, len(masks), 1))\n",
    "def plot_masks(row):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(masks[row][\"segmentation\"])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11a7c0",
   "metadata": {},
   "source": [
    "The following couple cells will tag masks that are clearly off and then they can be removed from the masks variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a47f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_to_be_removed(masks, area_threshold=0.4, max_number_big_objects=1):\n",
    "    to_be_removed = []\n",
    "    for n, mask, in enumerate(masks):\n",
    "        if mask[\"area_coverage\"] > area_threshold:\n",
    "            to_be_removed.append(n)\n",
    "        if mask[\"number_of_objects\"][\"big\"] > max_number_big_objects:\n",
    "            to_be_removed.append(n)\n",
    "    return to_be_removed\n",
    "\n",
    "to_be_removed = get_masks_to_be_removed(masks)\n",
    "to_be_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(to_be_removed) > 0:\n",
    "    masks.pop(to_be_removed.pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5d4bc",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "After removing the extra masks that correspond to mistakes or background, we can combine every mask to find an estimation of the seabed complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c5dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = np.zeros_like(masks[0][\"segmentation\"])\n",
    "for mask in masks:\n",
    "    background += mask[\"segmentation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f3af",
   "metadata": {},
   "source": [
    "By combining the background contour and the points annotated, we can roughly see where the model had problems.\n",
    "Some of the points might be slightly off the label of interest, which is problematic for automatic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.contour(background)\n",
    "plt.scatter(df_annotations.x, df_annotations.y, s=4, c=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
